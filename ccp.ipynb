{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3256,
     "status": "ok",
     "timestamp": 1760514708765,
     "user": {
      "displayName": "Rohith KR",
      "userId": "02249424468865075992"
     },
     "user_tz": -330
    },
    "id": "XQ1ALLXWBvN7",
    "outputId": "e89894e0-0990-4ee4-ebfa-71ef9d102bed"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, confusion_matrix, classification_report, log_loss\n",
    "from scipy.special import expit\n",
    "import json\n",
    "\n",
    "file_path = \"netflix_customer_churn.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "print(\"Dataset Loaded Successfully \")\n",
    "print(\"Shape:\", df.shape)\n",
    "print(df.head())\n",
    "\n",
    "possible_labels = ['churn', 'Churn', 'Churn?', 'Exited', 'is_churn', 'Cancelled', 'CustomerChurn']\n",
    "label_col = None\n",
    "for c in df.columns:\n",
    "    if c.lower() in [x.lower() for x in possible_labels]:\n",
    "        label_col = c\n",
    "        break\n",
    "if label_col is None:\n",
    "    for c in df.columns:\n",
    "        if 'churn' in c.lower() or 'exit' in c.lower() or 'cancel' in c.lower():\n",
    "            label_col = c\n",
    "            break\n",
    "if label_col is None:\n",
    "    raise ValueError(\"Could not detect churn label column. Please rename the target column to 'Churn' or similar.\")\n",
    "\n",
    "print(\"Detected target column:\", label_col)\n",
    "\n",
    "\n",
    "def to_binary(series):\n",
    "    mapping = {'yes':1,'y':1,'true':1,'t':1,'1':1,'no':0,'n':0,'false':0,'f':0,'0':0}\n",
    "    s = series.astype(str).str.lower().str.strip()\n",
    "    return s.map(mapping).fillna(0).astype(int)\n",
    "\n",
    "y = to_binary(df[label_col])\n",
    "X = df.drop(columns=[label_col])\n",
    "\n",
    "id_cols = [c for c in X.columns if 'id' in c.lower() and X[c].nunique()==len(X)]\n",
    "X = X.drop(columns=id_cols, errors='ignore')\n",
    "\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = [c for c in X.select_dtypes(exclude=[np.number]).columns if X[c].nunique() <= 50]\n",
    "X[num_cols] = X[num_cols].fillna(X[num_cols].median())\n",
    "X[cat_cols] = X[cat_cols].fillna('missing').astype(str)\n",
    "\n",
    "X = pd.get_dummies(X, columns=cat_cols, drop_first=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "print(\"Train/Test Split Done \")\n",
    "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
    "\n",
    "K = 5\n",
    "indices = np.arange(len(X_train))\n",
    "np.random.shuffle(indices)\n",
    "parts = np.array_split(indices, K)\n",
    "client_data = [(X_train.iloc[p], y_train.iloc[p]) for p in parts]\n",
    "\n",
    "classes = np.array([0,1])\n",
    "global_coef = np.zeros((1, X_train.shape[1]))\n",
    "global_intercept = np.zeros(1)\n",
    "\n",
    "n_rounds = 10\n",
    "local_epochs = 3\n",
    "alpha = 0.0001\n",
    "\n",
    "val_accs = []\n",
    "val_losses = []\n",
    "\n",
    "for rnd in range(n_rounds):\n",
    "    client_coefs, client_intercepts, client_sizes = [], [], []\n",
    "    for Xi, yi in client_data:\n",
    "        mdl = SGDClassifier(loss='log_loss', alpha=alpha, max_iter=1, tol=None, warm_start=True)\n",
    "        mdl.partial_fit(Xi.values[:2], yi.values[:2], classes=classes)\n",
    "        mdl.coef_ = global_coef.copy()\n",
    "        mdl.intercept_ = global_intercept.copy()\n",
    "        for _ in range(local_epochs):\n",
    "            mdl.partial_fit(Xi.values, yi.values, classes=classes)\n",
    "        client_coefs.append(mdl.coef_)\n",
    "        client_intercepts.append(mdl.intercept_)\n",
    "        client_sizes.append(len(yi))\n",
    "    total_samples = np.sum(client_sizes)\n",
    "    global_coef = np.sum([c*s for c, s in zip(client_coefs, client_sizes)], axis=0) / total_samples\n",
    "    global_intercept = np.sum([i*s for i, s in zip(client_intercepts, client_sizes)], axis=0) / total_samples\n",
    "\n",
    "    logits = X_test.values.dot(global_coef.T).ravel() + global_intercept.ravel()\n",
    "    probs = expit(logits)\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    loss = log_loss(y_test, probs)\n",
    "    val_accs.append(acc)\n",
    "    val_losses.append(loss)\n",
    "    print(f\"Round {rnd+1}/{n_rounds}: Accuracy={acc:.4f}, LogLoss={loss:.4f}\")\n",
    "\n",
    "logits = X_test.values.dot(global_coef.T).ravel() + global_intercept.ravel()\n",
    "probs = expit(logits)\n",
    "preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "acc = accuracy_score(y_test, preds)\n",
    "auc = roc_auc_score(y_test, probs)\n",
    "print(\"\\n Final Model Performance\")\n",
    "print(\"Accuracy:\", round(acc, 4))\n",
    "print(\"ROC AUC:\", round(auc, 4))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, preds))\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(range(1, n_rounds+1), val_accs, marker='o', label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Federated Round\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Federated Round-wise Accuracy\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(range(1, n_rounds+1), val_losses, marker='o', color='red', label=\"Validation Log Loss\")\n",
    "plt.xlabel(\"Federated Round\")\n",
    "plt.ylabel(\"Log Loss\")\n",
    "plt.title(\"Validation Log Loss per Round\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, probs)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {auc:.3f}\")\n",
    "plt.plot([0,1],[0,1],'--',color='gray')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.imshow(cm, cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.colorbar()\n",
    "plt.xticks([0,1], [\"Predicted 0\",\"Predicted 1\"])\n",
    "plt.yticks([0,1], [\"Actual 0\",\"Actual 1\"])\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j, i, cm[i,j], ha=\"center\", va=\"center\", color=\"black\", fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "model_info = {\n",
    "    'coef': global_coef.ravel().tolist(),\n",
    "    'intercept': global_intercept.ravel().tolist(),\n",
    "    'feature_names': X.columns.tolist()\n",
    "}\n",
    "with open(\"federated_model_info.json\", \"w\") as f:\n",
    "    json.dump(model_info, f)\n",
    "print(\"Model coefficients saved to federated_model_info.json \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2150,
     "status": "ok",
     "timestamp": 1761495455295,
     "user": {
      "displayName": "Rohith KR",
      "userId": "02249424468865075992"
     },
     "user_tz": -330
    },
    "id": "gTOU82rdT1ma",
    "outputId": "04afc3e1-7e0b-4874-b569-135b046dcd2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: The file 'netflix_customer_churn.csv' was not found. Using mock data for demonstration.\n",
      "Round 1/10: Accuracy=0.5000, LogLoss=7.3052\n",
      "Round 2/10: Accuracy=0.5350, LogLoss=5.2290\n",
      "Round 3/10: Accuracy=0.4750, LogLoss=10.3164\n",
      "Round 4/10: Accuracy=0.5550, LogLoss=7.4042\n",
      "Round 5/10: Accuracy=0.4800, LogLoss=7.8545\n",
      "Round 6/10: Accuracy=0.4700, LogLoss=8.3237\n",
      "Round 7/10: Accuracy=0.4850, LogLoss=7.1941\n",
      "Round 8/10: Accuracy=0.4850, LogLoss=7.8153\n",
      "Round 9/10: Accuracy=0.4900, LogLoss=8.4985\n",
      "Round 10/10: Accuracy=0.4950, LogLoss=9.4722\n",
      "\n",
      "Final federated model coefficients and preprocessing metadata saved to model_metadata.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from scipy.special import expit\n",
    "import json\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "file_path = \"netflix_customer_churn.csv\"\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{file_path}' was not found. Using mock data for demonstration.\")\n",
    "    # Create mock data structure based on common churn datasets for deployment\n",
    "    data = {\n",
    "        'CustomerID': range(1, 1001),\n",
    "        'Age': np.random.randint(18, 70, 1000),\n",
    "        'Subscription_Length_Months': np.random.randint(1, 60, 1000),\n",
    "        'Monthly_Bill': np.random.uniform(10, 150, 1000),\n",
    "        'Total_Usage_Hours': np.random.uniform(50, 500, 1000),\n",
    "        'Gender': np.random.choice(['Male', 'Female'], 1000, p=[0.5, 0.5]),\n",
    "        'Plan': np.random.choice(['Basic', 'Standard', 'Premium'], 1000, p=[0.4, 0.4, 0.2]),\n",
    "        'Payment_Method': np.random.choice(['Credit Card', 'Bank Transfer', 'PayPal'], 1000, p=[0.5, 0.3, 0.2]),\n",
    "        'Device_Type': np.random.choice(['Mobile', 'Desktop', 'Smart TV'], 1000, p=[0.5, 0.3, 0.2]),\n",
    "        'CustomerChurn': np.random.randint(0, 2, 1000) # Target\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "# --- 2. Identify Target Column and Prepare Data ---\n",
    "possible_labels = ['churn', 'Churn', 'Churn?', 'Exited', 'is_churn', 'Cancelled', 'CustomerChurn']\n",
    "label_col = None\n",
    "for c in df.columns:\n",
    "    if c.lower() in [x.lower() for x in possible_labels]:\n",
    "        label_col = c\n",
    "        break\n",
    "if label_col is None:\n",
    "    raise ValueError(\"Could not detect churn label column. Please rename the target column to 'Churn' or similar.\")\n",
    "\n",
    "def to_binary(series):\n",
    "    mapping = {'yes':1,'y':1,'true':1,'t':1,'1':1,'no':0,'n':0,'false':0,'f':0,'0':0}\n",
    "    s = series.astype(str).str.lower().str.strip()\n",
    "    return s.map(mapping).fillna(0).astype(int)\n",
    "\n",
    "y = to_binary(df[label_col])\n",
    "X = df.drop(columns=[label_col])\n",
    "\n",
    "id_cols = [c for c in X.columns if 'id' in c.lower() and X[c].nunique()==len(X)]\n",
    "X = X.drop(columns=id_cols, errors='ignore')\n",
    "\n",
    "# --- 3. Preprocessing Metadata Capture ---\n",
    "num_cols_original = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols_original = [c for c in X.select_dtypes(exclude=[np.number]).columns if X[c].nunique() <= 50]\n",
    "\n",
    "# Impute missing values (as per original script)\n",
    "X[num_cols_original] = X[num_cols_original].fillna(X[num_cols_original].median())\n",
    "X[cat_cols_original] = X[cat_cols_original].fillna('missing').astype(str)\n",
    "\n",
    "# One-hot encoding (drop_first=True, as per original script)\n",
    "X_processed = pd.get_dummies(X, columns=cat_cols_original, drop_first=True)\n",
    "\n",
    "# Capture one-hot mapping for the front-end\n",
    "one_hot_map = {}\n",
    "for col in cat_cols_original:\n",
    "    # Identify which new dummy columns belong to this original column\n",
    "    dummy_cols = [c for c in X_processed.columns if c.startswith(f'{col}_')]\n",
    "\n",
    "    # Get the unique categories (excluding the first one, which was dropped)\n",
    "    categories = X[col].unique().tolist()\n",
    "\n",
    "    # Store the mapping: Original_Value -> Processed_Column_Name\n",
    "    # The first value (base case) maps to no column (all zeros)\n",
    "    base_category = sorted(categories)[0] # Assuming alphabetically first is the dropped one\n",
    "\n",
    "    # Map all categories except the base one to their respective dummy column\n",
    "    for cat in categories:\n",
    "        if cat != base_category:\n",
    "            dummy_col_name = f\"{col}_{cat}\"\n",
    "            if dummy_col_name in dummy_cols:\n",
    "                one_hot_map[f\"{col}__{cat}\"] = dummy_col_name\n",
    "        else:\n",
    "             # This is the reference category (all dummy columns for this feature are 0)\n",
    "            one_hot_map[f\"{col}__{cat}\"] = \"REFERENCE\"\n",
    "\n",
    "# --- 4. Scaling (StandardScaler fitted on *all* processed features) ---\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_processed)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X_processed.columns)\n",
    "\n",
    "# --- 5. Model Training (Federated Averaging Logic) ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "K = 5\n",
    "indices = np.arange(len(X_train))\n",
    "np.random.shuffle(indices)\n",
    "parts = np.array_split(indices, K)\n",
    "client_data = [(X_train.iloc[p], y_train.iloc[p]) for p in parts]\n",
    "\n",
    "classes = np.array([0,1])\n",
    "global_coef = np.zeros((1, X_train.shape[1]))\n",
    "global_intercept = np.zeros(1)\n",
    "\n",
    "n_rounds = 10\n",
    "local_epochs = 3\n",
    "alpha = 0.0001\n",
    "\n",
    "for rnd in range(n_rounds):\n",
    "    client_coefs, client_intercepts, client_sizes = [], [], []\n",
    "    for Xi, yi in client_data:\n",
    "        mdl = SGDClassifier(loss='log_loss', alpha=alpha, max_iter=1, tol=None, warm_start=True, random_state=42)\n",
    "        # partial_fit needs to be called once with all classes to initialize\n",
    "        mdl.partial_fit(Xi.values[:2], yi.values[:2], classes=classes)\n",
    "        mdl.coef_ = global_coef.copy()\n",
    "        mdl.intercept_ = global_intercept.copy()\n",
    "        for _ in range(local_epochs):\n",
    "            mdl.partial_fit(Xi.values, yi.values, classes=classes)\n",
    "        client_coefs.append(mdl.coef_)\n",
    "        client_intercepts.append(mdl.intercept_)\n",
    "        client_sizes.append(len(yi))\n",
    "\n",
    "    total_samples = np.sum(client_sizes)\n",
    "    global_coef = np.sum([c*s for c, s in zip(client_coefs, client_sizes)], axis=0) / total_samples\n",
    "    global_intercept = np.sum([i*s for i, s in zip(client_intercepts, client_sizes)], axis=0) / total_samples\n",
    "\n",
    "    logits = X_test.values.dot(global_coef.T).ravel() + global_intercept.ravel()\n",
    "    probs = expit(logits)\n",
    "    acc = accuracy_score(y_test, (probs >= 0.5).astype(int))\n",
    "    loss = log_loss(y_test, probs)\n",
    "    print(f\"Round {rnd+1}/{n_rounds}: Accuracy={acc:.4f}, LogLoss={loss:.4f}\")\n",
    "\n",
    "# --- 6. Save Model and Metadata ---\n",
    "\n",
    "# Ensure scaler mean_ and scale_ match the shape of the features\n",
    "if len(scaler.mean_) != X_scaled.shape[1]:\n",
    "    raise RuntimeError(\"Scaler dimensions mismatch with final features.\")\n",
    "\n",
    "model_metadata = {\n",
    "    'coef': global_coef.ravel().tolist(),\n",
    "    'intercept': global_intercept.ravel().tolist(),\n",
    "    'feature_names_processed': X_scaled.columns.tolist(), # The 1D list of all feature names for the model\n",
    "    'scaler': {\n",
    "        'mean': scaler.mean_.tolist(),\n",
    "        'std': scaler.scale_.tolist()\n",
    "    },\n",
    "    'original_features': {\n",
    "        'numerical': num_cols_original,\n",
    "        'categorical': cat_cols_original\n",
    "    },\n",
    "    'categorical_map': one_hot_map\n",
    "}\n",
    "\n",
    "with open(\"model_metadata.json\", \"w\") as f:\n",
    "    json.dump(model_metadata, f, indent=4)\n",
    "print(\"\\nFinal federated model coefficients and preprocessing metadata saved to model_metadata.json\")\n",
    "\n",
    "# --- End of Python Script ---\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOWEHEY0mEn7mNkuPZXGqPn",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
